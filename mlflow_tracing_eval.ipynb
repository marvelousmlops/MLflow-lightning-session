{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe6d7ae6-03a5-41cd-a81c-5dea80fe9ea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Trace, Evaluate, Iterate: Building Reliable AI Systems with MLflow\n",
    "\n",
    "In this demo, we will show how to use MLflow to trace and evaluate an AI social media style transfer system. MLflow provides a suite of tools for GenAI application development.\n",
    "\n",
    "- **Different combinations of models, prompts, and inference parameters can have huge impacts on a GenAI application's performance.**\n",
    "- **It can be difficult to define what makes a GenAI model's responses \"good\" or \"bad,\" making it challenging to evaluate GenAI application performance.**\n",
    "- **Identifying sources of errors or breakdowns in application logic is very difficult when dealing with complex GenAI applications.**\n",
    "\n",
    "We will see how MLflow can be used to address these common challenges in GenAI application development.\n",
    "\n",
    "## Problem Setup: Generating Social Media Posts in Your Voice with AI\n",
    "- By default, AI models will generate writing that does not sound like it was written by you or by your organization.\n",
    "- In many cases, it will obviously be AI generated.\n",
    "- It might also contain significant inaccuracies\n",
    "\n",
    "In this notebook, we will use GenAI models to write social media posts in a specified voice and style. We will use MLflow to keep track of our experiments, make sure the posts are rigorously grounded in source material, and evaluate how well they correspond to the target style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d72dd389-37d5-48fd-98c5-0b5b7c615a5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup\n",
    "- Install Packages\n",
    "- We will be using models available through the Foundation Models API, so no need to configure GenAI model provider API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f372809-518e-4857-9fee-b2a1b9d9688d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q mlflow==2.21.3 openai==1.75.0 requests==2.31.0 markdownify\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dad223e-0ec1-4397-8445-494172e73994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generating our First Post\n",
    "We will need:\n",
    "\n",
    "- An **example post** to use as a style reference (eventually, we will use a set of multiple examples)\n",
    "- A **source document or website** with content to use as a source of information for a new post. We will write a helper function that can pull the text from a webpage and convert it to markdown.\n",
    "- A **template prompt** to tie the above information together\n",
    "- A **system prompt** explaining the task to the model\n",
    "\n",
    "Let's try the following. Feel free to experiment with your own prompts and ideas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "166cc97e-8522-4b86-b72e-cf592a249960",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from markdownify import markdownify\n",
    "\n",
    "system_prompt_1 = \"\"\"You are a social media content specialist who can precisely match writing styles. Your task is to:\n",
    "1. Analyze the provided example post(s) to understand their style and tone\n",
    "2. Generate a new LinkedIn post about the given topic that perfectly matches this style\n",
    "3. Return only the generated post, nothing else.\n",
    "\"\"\"\n",
    "\n",
    "user_template = \"\"\"\n",
    "example posts:\n",
    "{example_posts}\n",
    "\n",
    "topic:\n",
    "{topic}\n",
    "\n",
    "additional instructions:\n",
    "{additional_instructions}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Helper function to convert a webpage to markdown\n",
    "def webpage_to_markdown(url):\n",
    "    # Get webpage content\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    markdown_content = markdownify(html_content)\n",
    "\n",
    "    return markdown_content\n",
    "\n",
    "# Function to assemble the prompt\n",
    "def generate_prompt(\n",
    "    system, user_template, example_posts, topic, additional_instructions\n",
    "):\n",
    "    \"\"\"Generate a prompt for the LLM based on the example posts, topic, and additional instructions.\"\"\"\n",
    "    example_posts = \"\\n\".join(\n",
    "        [f\"Example {i+1}:\\n{post}\" for i, post in enumerate(example_posts)]\n",
    "    )\n",
    "    prompt = user_template.format(\n",
    "        example_posts=example_posts,\n",
    "        topic=topic,\n",
    "        additional_instructions=additional_instructions,\n",
    "    )\n",
    "\n",
    "    formatted_prompt = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    return formatted_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e860e036-72a3-4c1a-a796-28357cd6cbe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Querying an AI Model‚ÄîWithout Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ae4a6a-fa3e-403c-bd28-0bded15d1929",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://mlflow.org/docs/latest/llms/chat-model-intro/index.html\"\n",
    "markdown_content = webpage_to_markdown(url)\n",
    "\n",
    "additional_instructions = \"\"\"This post will be written for the MLflow LinkedIn account.\n",
    "Maintain a professional but approachable tone. Developers are the primary audience, so keep \n",
    "the content technically focused but accessible.\"\"\"\n",
    "\n",
    "example_post = \"\"\"MLflow's GenAI evaluation metrics now work as callable functions as of MLflow 2.17, making them easier to use and integrate.\n",
    "\n",
    "Now you can use metrics like answer_relevance, answer_correctness, faithfulness, and toxicity directly as functions‚Äîno need to go through mlflow.evaluate() anymore if you're just prototyping with individual metrics or integrating metric calls into systems where mlflow.evalaute is not necessary.\n",
    "\n",
    "This means:\n",
    "\n",
    "üîç Easier debugging during prototyping\n",
    "üîå More flexible integration options\n",
    "üéØ Works with or without other MLflow features\n",
    "\n",
    "Learn more:\n",
    "üìö Docs: https://lnkd.in/gyBzcrDr\n",
    "üìù Release notes: https://lnkd.in/gBrNQfFC\n",
    "\n",
    "#MachineLearning #AI #LLMs #LLMOps #Evals\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcffc2b5-ea0d-48bc-a5d6-393ba19810a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from openai import OpenAI\n",
    "\n",
    "SERVING_ENDPOINT = \"https://<your-workspace-name>.cloud.databricks.com/serving-endpoints\"\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Create a temporary token\n",
    "tmp_token = w.tokens.create(lifetime_seconds=1200).token_value\n",
    "\n",
    "# Set up the OpenAI client to use Databricks Model Serving\n",
    "client = OpenAI(\n",
    "    api_key=tmp_token,\n",
    "    base_url=SERVING_ENDPOINT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2f2691d-eafb-42e7-9562-842f7b59679c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "messages = generate_prompt(\n",
    "    system_prompt_1,\n",
    "    user_template,\n",
    "    [example_post],\n",
    "    [markdown_content],\n",
    "    additional_instructions,\n",
    ")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"databricks-llama-4-maverick\",\n",
    "    # model = \"databricks-meta-llama-3-3-70b-instruct\",\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1055f32-b319-4698-ad7d-c1d359c854d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### With Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ff3f2cb-e9ae-41c9-bc54-fa65076cdb5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "EXPERIMENT_NAME = \"<your-experiment-name>\"\n",
    "\n",
    "mlflow.openai.autolog()\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"databricks-llama-4-maverick\",\n",
    "    messages=messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d9c0149-6eb0-47a2-9f05-5f6c11b09dc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Capture the full execution flow\n",
    "\n",
    "We are not limited to tracing the AI inputs and outputs. We can also trace the `webpage_to_markdown` and `generate_prompt` functions. This can help us to identify bugs anywhere in the execution flow.\n",
    "\n",
    "Let's update these functions by adding the `@mlflow.trace` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "425ccfac-b390-48fd-9441-acc633633f0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# trace the webpage_to_markdown function\n",
    "@mlflow.trace(span_type=\"FUNCTION\")\n",
    "def webpage_to_markdown(url):\n",
    "    # Get webpage content\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    # Convert to markdown\n",
    "    markdown_content = markdownify(html_content)\n",
    "\n",
    "    return markdown_content\n",
    "\n",
    "\n",
    "# trace the generate_prompt function\n",
    "@mlflow.trace(span_type=\"FUNCTION\")\n",
    "def generate_prompt(\n",
    "    system, user_template, example_posts, topic, additional_instructions\n",
    "):\n",
    "    \"\"\"Generate a prompt for the LLM based on the example posts, topic, and additional instructions.\"\"\"\n",
    "    example_posts = \"\\n\".join(\n",
    "        [f\"Example {i+1}:\\n{post}\" for i, post in enumerate(example_posts)]\n",
    "    )\n",
    "    prompt = user_template.format(\n",
    "        example_posts=example_posts,\n",
    "        topic=topic,\n",
    "        additional_instructions=additional_instructions,\n",
    "    )\n",
    "\n",
    "    formatted_prompt = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    return formatted_prompt\n",
    "\n",
    "\n",
    "# create a parent span\n",
    "with mlflow.start_span(name=\"Generate Post\", span_type=\"CHAIN\") as span:\n",
    "    markdown_content = webpage_to_markdown(url)\n",
    "    messages = generate_prompt(\n",
    "        system_prompt_1,\n",
    "        user_template,\n",
    "        [example_post],\n",
    "        [markdown_content],\n",
    "        additional_instructions,\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"databricks-llama-4-maverick\",\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a0bf2a8-b3f0-4e8d-abc4-6782031a96e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluating the Generated Post\n",
    "- Suppose during our experimentation we found that a couple of different prompts and a couple of different models were especially promising, and we want to compare them to detemrine which combination is best.\n",
    "- Factual correctness and style similarity are our highest priorities.\n",
    "- We will use the built-in `faithfulness` metric to make sure the generated texts are true to the source texts.\n",
    "- We will create a custom `style_similarity_metric` to measure the similarity between the generated post and the example posts.\n",
    "- Both of these are *llm-as-judge* metrics, meaning that another large language model is used to grade the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39c5c7c9-2b0d-48ba-bcd1-91b6b1f55817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Built-in metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b17d82d-f233-498f-9655-9fd776cb7375",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.metrics.genai import faithfulness\n",
    "\n",
    "faithfulness_metric = faithfulness(model=\"endpoints:/databricks-claude-3-7-sonnet\")\n",
    "\n",
    "result = response.choices[0].message.content\n",
    "\n",
    "print(\n",
    "    faithfulness_metric(\n",
    "        predictions=result,\n",
    "        inputs=additional_instructions,  # ignored\n",
    "        context=markdown_content,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00fedb59-381f-4cfa-b672-95eaa0be6a65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Custom Metric: Style Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6978daa-39c6-43a3-90d4-4c7064a5a552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "post_example_1 = \"\"\"MLflow's GenAI evaluation metrics now work as callable functions as of MLflow 2.17, making them easier to use and integrate.\n",
    "\n",
    "Now you can use metrics like answer_relevance, answer_correctness, faithfulness, and toxicity directly as functions‚Äîno need to go through mlflow.evaluate() anymore if you're just prototyping with individual metrics or integrating metric calls into systems where mlflow.evalaute is not necessary.\n",
    "\n",
    "This means:\n",
    "\n",
    "üîç Easier debugging during prototyping\n",
    "üîå More flexible integration options\n",
    "üéØ Works with or without other MLflow features\n",
    "\n",
    "Check it out in action ‚¨áÔ∏è\n",
    "\n",
    "Learn more:\n",
    "üìö Docs: https://lnkd.in/gyBzcrDr\n",
    "üìù Release notes: https://lnkd.in/gBrNQfFC\n",
    "\n",
    "#MachineLearning #AI #LLMs #LLMOps #Evals\"\"\"\n",
    "\n",
    "post_example_2 = \"\"\"If you're already building with Python ML libraries, adding mlflow.autolog() to your code instantly gives you production-grade experiment tracking, model management, and observability‚Äîno extra infrastructure or code changes needed.\n",
    "\n",
    "The automatic logging works across a remarkable breadth of libraries‚Äîfrom GenAI frameworks like LangChain, OpenAI, and LlamaIndex to traditional ML and deep learning libraries like PyTorch, scikit-learn, and Fastai.\n",
    "\n",
    "MLflow's autolog feature changes this equation. With a single line‚Äîmlflow.autolog()‚Äîyou get automatic logging of:\n",
    "\n",
    "üìä Training metrics and parameters for scikit-learn, PyTorch, many other ML frameworks\n",
    "üîÑ LLM traces, prompts, responses, and tool calls for OpenAI and LangChain\n",
    "üì¶ Model signatures and artifacts\n",
    "üíæ Dataset information and example inputs\n",
    "\n",
    "The best part is that it works out of the box with the most popular libraries in the Python ML ecosystem: no need to modify your existing training code or add manual logging statements.\n",
    "\n",
    "Read more: https://lnkd.in/e_aTp6HH\n",
    "\n",
    "#machinelearning #mlops #ai #llmops\"\"\"\n",
    "\n",
    "post_example_3 = \"\"\"New tutorial: Step-by-step guide to building a tool-calling LLM application using MLflow's ChatModel wrapper and tracing system.\n",
    "\n",
    "This tutorial shows you how to:\n",
    "\n",
    "üîß Create a tool-calling model using mlflow.pyfunc.ChatModel\n",
    "üîÑ Implement OpenAI function calling with automatic input/output handling\n",
    "üîç Add comprehensive tracing to debug multi-step LLM interactions\n",
    "üöÄ Deploy your model with full MLflow lifecycle management\n",
    "\n",
    "The guide includes a practical example building a weather information agent, showing how ChatModel simplifies complex LLM patterns while providing enterprise-grade observability.\n",
    "\n",
    "Check out the complete tutorial here: https://lnkd.in/gdTw8N2S\n",
    "\n",
    "#MLOps #AIEngineering #LLMOps #AI\"\"\"\n",
    "\n",
    "example_posts = [post_example_1, post_example_2, post_example_3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1e48c04-4dbb-4431-8142-65c9af16d1fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Define Evaluation Examples\n",
    "- Evaluation examples give the LLM judge guidance on high- and low-scoring outputs.\n",
    "- Here, we will generate two examples: one following the style of the example posts (technical but approachable) and one with an over-the-top tone and hype-oriented language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62bc7afa-51b5-4b5a-bc14-bebd3b481813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.metrics.genai import EvaluationExample\n",
    "\n",
    "similar_post = \"\"\"\n",
    "MLflow's ChatModel and PythonModel classes serve different needs when deploying GenAI applications. Here's when to use each:\n",
    "\n",
    "ChatModel simplifies GenAI deployment with standardized OpenAI-compatible interfaces. This means:\n",
    "\n",
    "üîó Immediate compatibility with existing OpenAI-based tools and workflows\n",
    "üöÄ Pre-defined model signatures that work out of the box\n",
    "üìä Streamlined integration with MLflow's tracking and evaluation features\n",
    "\n",
    "PythonModel is your choice when you need complete control over:\n",
    "\n",
    "üõ†Ô∏è Custom input/output schemas for specialized use cases\n",
    "üîÑ Complex data transformations beyond standard chat patterns\n",
    "‚öôÔ∏è Fine-grained model behavior and deployment configurations\n",
    "\n",
    "For most conversational AI applications, ChatModel's standardized approach helps you avoid common deployment pitfalls while maintaining consistent interfaces across your GenAI services. Consider PythonModel when your use case requires specialized data handling or custom interaction patterns.\n",
    "\n",
    "See the comment below for links to in-depth tutorials on ChatModel üëá \n",
    "\n",
    "#MLflow #LLMOps #MachineLearning #GenAI\"\"\"\n",
    "\n",
    "dissimilar_post = \"\"\"\n",
    "üî• HOLY MOLY! MLflow Just Dropped Something INSANE for AI Deployment! ü§Ø\n",
    "TWO EPIC WAYS to deploy your next-gen AI:\n",
    "1Ô∏è‚É£ ChatModel: The No-BS Fast Track!\n",
    "\n",
    "INSTANT OpenAI compatibility ü§ù\n",
    "Zero headaches, works RIGHT NOW üöÄ\n",
    "All the tracking & metrics you're craving üìà\n",
    "\n",
    "2Ô∏è‚É£ PythonModel: For When You Need to GO WILD!\n",
    "\n",
    "Customize EVERYTHING üé®\n",
    "Transform data like a BOSS üí™\n",
    "Ultimate control = Ultimate POWER! ‚ö°Ô∏è\n",
    "\n",
    "Don't sleep on this update! Your AI deployment game is about to get ABSOLUTELY CRACKED! üöÄ‚ú®\n",
    "#MLflowGang #AIrevolution #FutureIsNow #TechTwitter\n",
    "\"\"\"\n",
    "\n",
    "evaluation_example_1 = EvaluationExample(\n",
    "    input=additional_instructions,\n",
    "    output=similar_post,\n",
    "    grading_context={\"examples\": example_posts},\n",
    "    score=5,\n",
    "    justification=\"This post is a perfect match to the style of the example posts.\"\n",
    ")\n",
    "\n",
    "evaluation_example_2 = EvaluationExample(\n",
    "    input=additional_instructions,\n",
    "    output=dissimilar_post,\n",
    "    grading_context={\"examples\": example_posts},\n",
    "    score=1,\n",
    "    justification=(\"The post earns a 1/5 for maintaining the basic bullet-point structure and use of emojis, \"\n",
    "                   \"but significantly overplays the informal tone with phrases like 'HOLY MOLY!' and 'fam' \"\n",
    "                   \"that aren't present in the examples. While the example posts balance professional \"\n",
    "                   \"enthusiasm with technical detail, this submission sacrifices information density for \"\n",
    "                   \"excessive hype and casual language that goes well beyond the controlled informality shown \"\n",
    "                   \"in the reference posts.\" )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5962e96-635b-4f67-b8b5-7b2af7ace6e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Define the Custom Grading Rubric\n",
    "\n",
    "In this section, we use the `make_genai_metric` function to define our custom metric. Critically, we have to include the metric definition and grading prompt instructing the model how to grade the inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4b748ac-a2c4-46cf-8744-09d121fa7e7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.metrics.genai import make_genai_metric\n",
    "\n",
    "style_similarity_metric = make_genai_metric(\n",
    "    name=\"style_similarity\",\n",
    "    definition=(\n",
    "        \"Style similarity measures how well a generated social media post matches the style, tone, \"\n",
    "        \"and vocabulary of provided example posts. This includes analyzing the similarity of tone, \"\n",
    "        \"word choice, punctuation, sentence structure, and stylistic elements like hashtags and emojis. \"\n",
    "        \"Content similarity should not factor into the style similarity score. Post length is of minimal importance.\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"Style Similarity: Score the generated post's similarity to the example posts on a scale from 0 to 5:\\n\"\n",
    "        \"- Score 0: No stylistic similarity at all\\n\"\n",
    "        \"- Score 1: Minimal stylistic similarity\\n\"\n",
    "        \"- Score 2: Some stylistic elements match but significant differences exist\\n\"\n",
    "        \"- Score 3: Moderate stylistic similarity in tone, vocabulary, or structure\\n\"\n",
    "        \"- Score 4: High stylistic similarity across most elements\\n\"\n",
    "        \"- Score 5: Could be written by the same author\\n\\n\"\n",
    "        \"Consider:\\n\"\n",
    "        \"- Tone: similarity in voice and attitude\\n\"\n",
    "        \"- Vocabulary: similarity in word choice and complexity\\n\"\n",
    "        \"- Style: similarity in punctuation, sentence structure, hashtags, and emojis\"\n",
    "    ),\n",
    "    examples=[evaluation_example_1, evaluation_example_2],\n",
    "    version=\"v1\",\n",
    "    model=\"endpoints:/databricks-claude-3-7-sonnet\",\n",
    "    parameters={\"temperature\": 0.0, \"max_tokens\": 1000},\n",
    "    aggregations=[\"mean\", \"variance\", \"p90\"],\n",
    "    grading_context_columns=[\"examples\"],\n",
    "    greater_is_better=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb66ba81-bca3-4f71-9a60-de111486a254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Test our Custom Metric\n",
    "- MLflow metrics work as Python callables, so we can invoke the metric directly on an example to test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5811276b-1e28-4b04-b2cf-0bafdc10b5e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "too_formal_example = \"\"\"MLflow has introduced distinct deployment paradigms through its ChatModel and PythonModel classes, each serving specific implementation requirements in GenAI applications.\n",
    "ChatModel implements a standardized deployment framework utilizing OpenAI-compatible interfaces, offering several advantages:\n",
    "\n",
    "Full compatibility with existing OpenAI infrastructure and workflows\n",
    "Implementation of predefined model signatures ensuring immediate functionality\n",
    "Seamless integration with MLflow's comprehensive tracking and evaluation systems\n",
    "\n",
    "Conversely, PythonModel provides advanced customization capabilities for specialized requirements:\n",
    "\n",
    "Implementation of bespoke input/output schemas\n",
    "Advanced data transformation protocols beyond standard conversational patterns\n",
    "Granular control over model behavior and deployment specifications\n",
    "\n",
    "For standard conversational AI implementations, ChatModel's structured approach mitigates common deployment challenges while maintaining consistent interfaces across GenAI services. PythonModel remains the optimal choice for implementations requiring specialized data handling protocols or custom interaction patterns.\n",
    "For detailed implementation guidelines, please refer to the accompanying documentation.\n",
    "Reference: MLflow Documentation\n",
    "\"\"\"\n",
    "\n",
    "style_similarity_metric(\n",
    "    predictions=too_formal_example,\n",
    "    inputs=additional_instructions,\n",
    "    examples=[example_posts],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37564f90-d163-411e-84fd-7443bab6ffe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## `mlflow.evaluate` for structured experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1296b3a-39c9-4a82-92be-2fcd690ccbc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "system_prompt_2 = \"\"\"You are a social media content specialist with expertise in matching writing styles and voice across platforms. Your task is to:\n",
    "\n",
    "1. Analyze the provided example post(s) by examining:\n",
    "   - Writing style, tone, and voice\n",
    "   - Sentence structure and length\n",
    "   - Use of hashtags, emojis, and formatting\n",
    "   - Engagement techniques and calls-to-action\n",
    "\n",
    "2. Generate a new LinkedIn post about the given topic that matches:\n",
    "   - The identified writing style and tone\n",
    "   - Similar structure and formatting choices\n",
    "   - Equivalent use of platform features and hashtags\n",
    "   - Comparable engagement elements\n",
    "\n",
    "3. Return only the generated post, formatted exactly as it would appear on LinkedIn, without any additional commentary or explanations.\"\"\"\n",
    "\n",
    "system_prompts = {\"concise\": system_prompt_1, \"detailed\": system_prompt_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import mlflow\n",
    "import requests\n",
    "from markdownify import markdownify\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Create a temporary token\n",
    "tmp_token = w.tokens.create(lifetime_seconds=1200).token_value\n",
    "\n",
    "# Set up the OpenAI client to use Databricks Model Serving\n",
    "client = OpenAI(\n",
    "    api_key=tmp_token,\n",
    "    base_url=SERVING_ENDPOINT,\n",
    ")\n",
    "\n",
    "user_template = \"\"\"\n",
    "example posts:\n",
    "{example_posts}\n",
    "\n",
    "topic:\n",
    "{topic}\n",
    "\n",
    "\n",
    "additional instructions:\n",
    "{additional_instructions}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@mlflow.trace(span_type=\"FUNCTION\")\n",
    "def webpage_to_markdown(url):\n",
    "    # Get webpage content\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    # Convert to markdown\n",
    "    markdown_content = markdownify(html_content)\n",
    "\n",
    "    return markdown_content\n",
    "\n",
    "\n",
    "# trace the generate_prompt function\n",
    "@mlflow.trace(span_type=\"FUNCTION\")\n",
    "def generate_prompt(\n",
    "    system, user_template, example_posts, topic, additional_instructions\n",
    "):\n",
    "    \"\"\"Generate a prompt for the LLM based on the example posts, topic, and additional instructions.\"\"\"\n",
    "    example_posts = \"\\n\".join(\n",
    "        [f\"Example {i+1}:\\n{post}\" for i, post in enumerate(example_posts)]\n",
    "    )\n",
    "    prompt = user_template.format(\n",
    "        example_posts=example_posts,\n",
    "        topic=topic,\n",
    "        additional_instructions=additional_instructions,\n",
    "    )\n",
    "\n",
    "    formatted_prompt = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    return formatted_prompt\n",
    "\n",
    "\n",
    "def generate_evaluation_dataset(\n",
    "    system_prompts: dict,\n",
    "    client: openai.OpenAI,\n",
    "    models: list,\n",
    "    additional_instructions: str,\n",
    "    example_posts: list,\n",
    "):\n",
    "    mlflow_pages = {\n",
    "        \"Tutorial: Custom GenAI Models using ChatModel\": \"https://mlflow.org/docs/latest/llms/chat-model-guide/index.html\",\n",
    "        \"MLflow Tracing Schema\": \"https://mlflow.org/docs/latest/tracing/tracing-schema\",\n",
    "        \"MLflow AI Gateway (Experimental)\": \"https://mlflow.org/docs/latest/llms/deployments/index.html\",\n",
    "        \"MLflow LLM Evaluation\": \"https://mlflow.org/docs/latest/llms/llm-evaluate/index.html\",\n",
    "        \"LLM Evaluation with MLflow Example Notebook\": \"https://mlflow.org/docs/latest/llms/llm-evaluate/notebooks/question-answering-evaluation.html\",\n",
    "        \"MLflow Tracing for LLM Observability\": \"https://mlflow.org/docs/latest/tracing\",\n",
    "        \"Deep Learning\": \"https://mlflow.org/docs/latest/deep-learning/index.html\",\n",
    "        \"DSPy Quickstart\": \"https://mlflow.org/docs/latest/llms/dspy/notebooks/dspy_quickstart.html\",\n",
    "        \"Building Custom Python Function Models with MLflow\": \"https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/index.html\",\n",
    "        \"Quickstart with MLflow PyTorch Flavor\": \"https://mlflow.org/docs/latest/deep-learning/pytorch/quickstart/pytorch_quickstart.html\",\n",
    "    }\n",
    "    results = []\n",
    "\n",
    "    for page_name, page_url in mlflow_pages.items():\n",
    "        for prompt_name, system_prompt in system_prompts.items():\n",
    "            for model in models:\n",
    "                with mlflow.start_span(\n",
    "                    name=\"eval_dataset_generation\",\n",
    "                    span_type=\"CHAIN\",\n",
    "                ) as parent_span:\n",
    "                    parent_span.set_inputs(\n",
    "                        {\n",
    "                            \"model\": model,\n",
    "                            \"system_prompt\": system_prompt,\n",
    "                            \"example_post\": page_name,\n",
    "                        }\n",
    "                    )\n",
    "                    page_content = webpage_to_markdown(page_url)\n",
    "                    messages = generate_prompt(\n",
    "                        system_prompt,\n",
    "                        user_template,\n",
    "                        example_posts,\n",
    "                        page_content,\n",
    "                        additional_instructions,\n",
    "                    )\n",
    "\n",
    "                    response = client.chat.completions.create(\n",
    "                        model=model,\n",
    "                        messages=messages,\n",
    "                    )\n",
    "\n",
    "                    results.append(\n",
    "                        {\n",
    "                            \"model\": model,\n",
    "                            \"system_prompt\": prompt_name,\n",
    "                            \"context_page\": page_name,\n",
    "                            \"context_full\": page_content,\n",
    "                            \"additional_instructions\": additional_instructions,\n",
    "                            \"output\": response.choices[0].message.content,\n",
    "                            \"example_posts\": example_posts,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    parent_span.set_outputs(\n",
    "                        {\"output\": response.choices[0].message.content}\n",
    "                    )\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24bf582a-82fc-4eeb-807e-79bc2d71f451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TABLE_FULL_NAME = \"<catalog-name>.<schema-name>.<table-name>\"\n",
    "\n",
    "if not spark.catalog.tableExists(TABLE_FULL_NAME):\n",
    "    eval_data = generate_evaluation_dataset(\n",
    "        system_prompts=system_prompts,\n",
    "        client=client,\n",
    "        models=[\n",
    "            #\"databricks-llama-4-maverick\",\n",
    "            \"databricks-meta-llama-3-3-70b-instruct\",\n",
    "            \"databricks-meta-llama-3-1-8b-instruct\"],\n",
    "        additional_instructions=additional_instructions,\n",
    "        example_posts=example_posts,\n",
    "    )\n",
    "\n",
    "    # Convert the Pandas DataFrame to a PySpark DataFrame\n",
    "    spark_df = spark.createDataFrame(eval_data)\n",
    "\n",
    "    # Write the eval data to a Delta table\n",
    "    spark_df.write.format(\"delta\").saveAsTable(TABLE_FULL_NAME)\n",
    "else:\n",
    "    eval_data = spark.table(TABLE_FULL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd359a6-856c-4e80-9289-4b013fa09a8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluate the Generated Dataset for Faithfulness and Style Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbe1ce78-d827-4451-a945-9a7b4a691574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_data = spark.table(TABLE_FULL_NAME)\n",
    "display(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d82c40c-6565-4a27-aa7d-b79708086fbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import json\n",
    "\n",
    "# Assume eval_data, style_similarity_metric, faithfulness_metric are defined and valid\n",
    "\n",
    "run_name = f\"full-dataset-eval-{uuid.uuid4()}\"\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"Started run '{run_name}' ({run_id}).\")\n",
    "\n",
    "    # Evaluate the entire dataframe. Assumes success.\n",
    "    eval_results = mlflow.evaluate(\n",
    "        data=eval_data,\n",
    "        predictions=\"output\",\n",
    "        extra_metrics=[\n",
    "            style_similarity_metric,\n",
    "            faithfulness_metric,\n",
    "        ],\n",
    "        evaluators=\"default\",\n",
    "        evaluator_config={\n",
    "            \"col_mapping\": {\n",
    "                \"inputs\": \"additional_instructions\",\n",
    "                \"context\": \"context_full\",\n",
    "                \"examples\": \"example_posts\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    mlflow.log_param(\"evaluation_status\", \"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff3e7ae7-12bf-4394-903c-bb61509ddd88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(eval_results.tables['eval_results_table'])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MLflow Tracing and Evaluation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
